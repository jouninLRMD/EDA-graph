{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe08b91",
   "metadata": {},
   "source": [
    "# CASE DATASET ANALYSIS NOTEBOOK\n",
    "Created on Tue Oct  3 16:46:11 2023\n",
    "\n",
    "@author: lrm22005\n",
    "\n",
    "This notebook can be runned just using the paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91f773",
   "metadata": {},
   "source": [
    "## EDA_GRAPH FEATURES CLASSIFICATION WITH OUR LABELS\n",
    "\n",
    "**Data Loading and Preprocessing:**\n",
    "The dataset is loaded from a CSV file named 'EDA_graph_features.csv'.\n",
    "The 'subject' column is used to identify different subjects.\n",
    "\n",
    "**Classifier Definitions:**\n",
    "Several classifiers, such as Naive Bayes, K-Nearest Neighbors, Random Forest, AdaBoost, Gradient Boosting, Decision Tree, and Support Vector Machine, are defined along with their hyperparameter grids for grid search.\n",
    "\n",
    "**Subjects Identification:**\n",
    "The unique subjects present in the dataset are identified.\n",
    "\n",
    "**Cross-Validation Strategy (Leave-One-Subject-Out):**\n",
    "The GroupKFold is used as the cross-validation strategy with the number of splits equal to the number of unique subjects. This means that each subject will be left out as a test set in one iteration, while the others will be used for training.\n",
    "\n",
    "**Classifier Evaluation Loop:**\n",
    "The code iterates over each classifier defined earlier.\n",
    "    For each classifier:\n",
    "        It initializes variables for accuracy and balanced accuracy.\n",
    "        It initializes empty lists to store true labels and predicted labels across all test subjects.\n",
    "        It iterates over each unique subject:\n",
    "            Separates the data into training and test sets, where the current subject is the test set.\n",
    "            Performs a grid search to find the best hyperparameters for the classifier using the training data.\n",
    "            Trains the best classifier on the training data.\n",
    "            Makes predictions on the test data.\n",
    "            Appends the true and predicted labels for later evaluation.\n",
    "        Calculates the average accuracy and balanced accuracy across all subjects.\n",
    "        Prints the classification report, including precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e83d5d",
   "metadata": {},
   "source": [
    "## CLASSIFICATION USING THE 5 MOST RELEVANT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bde3d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb:\n",
      "Average Accuracy: 0.747\n",
      "Average Balanced Accuracy: 0.243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85     12527\n",
      "           1       0.00      0.00      0.00       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.38      0.27      0.32      1807\n",
      "           4       0.00      0.00      0.00       889\n",
      "\n",
      "    accuracy                           0.75     16470\n",
      "   macro avg       0.23      0.24      0.23     16470\n",
      "weighted avg       0.63      0.75      0.68     16470\n",
      "\n",
      "knn:\n",
      "Average Accuracy: 0.662\n",
      "Average Balanced Accuracy: 0.218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.84      0.80     12527\n",
      "           1       0.10      0.06      0.08       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.15      0.14      0.15      1807\n",
      "           4       0.13      0.04      0.06       889\n",
      "\n",
      "    accuracy                           0.66     16470\n",
      "   macro avg       0.23      0.22      0.22     16470\n",
      "weighted avg       0.61      0.66      0.63     16470\n",
      "\n",
      "rf:\n",
      "Average Accuracy: 0.734\n",
      "Average Balanced Accuracy: 0.212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.95      0.85     12527\n",
      "           1       0.02      0.00      0.00       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.27      0.09      0.13      1807\n",
      "           4       0.11      0.02      0.03       889\n",
      "\n",
      "    accuracy                           0.73     16470\n",
      "   macro avg       0.23      0.21      0.20     16470\n",
      "weighted avg       0.62      0.73      0.66     16470\n",
      "\n",
      "abc:\n",
      "Average Accuracy: 0.755\n",
      "Average Balanced Accuracy: 0.203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86     12527\n",
      "           1       0.00      0.00      0.00       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.25      0.03      0.05      1807\n",
      "           4       0.00      0.00      0.00       889\n",
      "\n",
      "    accuracy                           0.76     16470\n",
      "   macro avg       0.20      0.20      0.18     16470\n",
      "weighted avg       0.61      0.76      0.66     16470\n",
      "\n",
      "gbc:\n",
      "Average Accuracy: 0.726\n",
      "Average Balanced Accuracy: 0.210\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84     12527\n",
      "           1       0.02      0.00      0.00       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.25      0.09      0.13      1807\n",
      "           4       0.09      0.02      0.03       889\n",
      "\n",
      "    accuracy                           0.73     16470\n",
      "   macro avg       0.22      0.21      0.20     16470\n",
      "weighted avg       0.61      0.73      0.66     16470\n",
      "\n",
      "dt:\n",
      "Average Accuracy: 0.746\n",
      "Average Balanced Accuracy: 0.208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.86     12527\n",
      "           1       0.00      0.00      0.00       834\n",
      "           2       0.03      0.00      0.00       413\n",
      "           3       0.30      0.06      0.10      1807\n",
      "           4       0.04      0.00      0.00       889\n",
      "\n",
      "    accuracy                           0.75     16470\n",
      "   macro avg       0.23      0.21      0.19     16470\n",
      "weighted avg       0.62      0.75      0.66     16470\n",
      "\n",
      "svm:\n",
      "Average Accuracy: 0.749\n",
      "Average Balanced Accuracy: 0.203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86     12527\n",
      "           1       0.03      0.00      0.00       834\n",
      "           2       0.00      0.00      0.00       413\n",
      "           3       0.21      0.02      0.04      1807\n",
      "           4       0.14      0.01      0.01       889\n",
      "\n",
      "    accuracy                           0.75     16470\n",
      "   macro avg       0.23      0.20      0.18     16470\n",
      "weighted avg       0.61      0.75      0.66     16470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "# load the libraries that are required for this project:\n",
    "#---------------------------------------------------------------------------------------------\n",
    "import sys\n",
    "\n",
    "import time                     # Time is for estimating the computational time of every operation\n",
    "import pandas as pd\n",
    "import numpy as np              # NumPy is for numerical operations\n",
    "import matplotlib               # MatPlotLib is for making plots & figures\n",
    "import matplotlib.pyplot as plt # PyPlot is a subset of the library for making MATLAB-style plots\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "def grid_search(clf, params, X_train, y_train):\n",
    "    grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Load the dataset\n",
    "path = r'C:\\Users\\lrm22005\\OneDrive - University of Connecticut\\Research\\emotion_graph\\codes\\EDA-graph\\\\'\n",
    "data = pd.read_csv(path + '\\\\EDA_graph_features.csv')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "X = data[['total_load_centrality','total_harmonic_centrality','graph_number_of_cliqs','P_diameter','P_radius','subject']]\n",
    "y = data['class']\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'nb': (OneVsRestClassifier(GaussianNB()), {'estimator__var_smoothing': [1e-6, 1e-5, 1e-4]}),\n",
    "    'knn': (OneVsRestClassifier(KNeighborsClassifier()), {'estimator__n_neighbors': [1, 2, 3, 4, 5], 'estimator__weights': ['uniform', 'distance'], 'estimator__p': [1, 2]}),\n",
    "    'rf': (OneVsRestClassifier(RandomForestClassifier()), {'estimator__n_estimators': [100, 200], 'estimator__max_depth': [20, 30], 'estimator__random_state': [42]}),\n",
    "    'abc': (OneVsRestClassifier(AdaBoostClassifier()), {'estimator__n_estimators': [50, 100, 150], 'estimator__learning_rate': [0.1, 0.5]}),\n",
    "    'gbc': (OneVsRestClassifier(GradientBoostingClassifier()), {'estimator__n_estimators': [50, 100], 'estimator__learning_rate': [0.1, 1.0], 'estimator__max_depth': [40, 60]}),\n",
    "    'dt': (OneVsRestClassifier(DecisionTreeClassifier()), {'estimator__max_depth': [10, 30], 'estimator__min_samples_split': [2, 10], 'estimator__random_state': [42]}),\n",
    "    'svm': (OneVsRestClassifier(SVC()), {'estimator__C': [1, 10, 100], 'estimator__gamma': [0.1, 1, 10], 'estimator__kernel': ['rbf']})\n",
    "}\n",
    "\n",
    "# Define the subjects\n",
    "subjects = X['subject']\n",
    "unique_subjects = X['subject'].unique()\n",
    "\n",
    "X = X.drop('subject', axis=1)\n",
    "\n",
    "# Define the subjects\n",
    "\n",
    "# Define the cross-validation strategy (leave-one-subject-out)\n",
    "cv = GroupKFold(n_splits=len(unique_subjects))\n",
    "\n",
    "# Perform grid search for each classifier\n",
    "best_classifiers = {}\n",
    "for name, (clf, params) in classifiers.items():\n",
    "    avg_accuracy = 0.0\n",
    "    avg_balanced_accuracy = 0.0\n",
    "    \n",
    "    # Initialize variables for evaluation\n",
    "    y_test_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    # Iterate over each subject as a separate test fold\n",
    "    for subject in unique_subjects:\n",
    "        # Identify the indices for the current subject in both features and target labels\n",
    "        subject_indices = subjects == subject\n",
    "        X_test, y_test = X[subject_indices], y[subject_indices]\n",
    "        X_train, y_train = X[~subject_indices], y[~subject_indices]\n",
    "        \n",
    "        # Grid search for the current classifier\n",
    "        best_clf = grid_search(clf, params, X_train, y_train)\n",
    "        best_classifiers[name] = best_clf\n",
    "        \n",
    "        # Train the best classifier on the training data\n",
    "        best_clf.fit(X_train, y_train)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            y_pred = best_clf.predict(X_test)  \n",
    "            \n",
    "        # Append current test data and predictions for overall evaluation\n",
    "#         y_pred = best_clf.predict(X_test)\n",
    "        y_test_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_accuracy = accuracy_score(y_test_all, y_pred_all)\n",
    "    avg_balanced_accuracy = balanced_accuracy_score(y_test_all, y_pred_all)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.3f}\")\n",
    "    print(f\"Average Balanced Accuracy: {avg_balanced_accuracy:.3f}\")\n",
    "    print(classification_report(y_test_all, y_pred_all, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad1066",
   "metadata": {},
   "source": [
    "## TRADITIONAL METHODS CLASSIFICATION WITH OUR LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3166a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb:\n",
      "Average Accuracy: 0.670\n",
      "Average Balanced Accuracy: 0.166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80     11085\n",
      "           1       0.00      0.00      0.00       438\n",
      "           2       0.00      0.00      0.00       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.00      0.00      0.00       864\n",
      "           5       1.00      0.00      0.00      3415\n",
      "\n",
      "    accuracy                           0.67     16470\n",
      "   macro avg       0.28      0.17      0.13     16470\n",
      "weighted avg       0.66      0.67      0.54     16470\n",
      "\n",
      "knn:\n",
      "Average Accuracy: 0.553\n",
      "Average Balanced Accuracy: 0.161\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72     11085\n",
      "           1       0.03      0.01      0.02       438\n",
      "           2       0.00      0.00      0.00       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.04      0.03      0.03       864\n",
      "           5       0.19      0.16      0.17      3415\n",
      "\n",
      "    accuracy                           0.55     16470\n",
      "   macro avg       0.16      0.16      0.16     16470\n",
      "weighted avg       0.50      0.55      0.52     16470\n",
      "\n",
      "rf:\n",
      "Average Accuracy: 0.599\n",
      "Average Balanced Accuracy: 0.160\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.86      0.75     11085\n",
      "           1       0.00      0.00      0.00       438\n",
      "           2       0.00      0.00      0.00       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.01      0.00      0.00       864\n",
      "           5       0.18      0.10      0.13      3415\n",
      "\n",
      "    accuracy                           0.60     16470\n",
      "   macro avg       0.14      0.16      0.15     16470\n",
      "weighted avg       0.49      0.60      0.53     16470\n",
      "\n",
      "abc:\n",
      "Average Accuracy: 0.673\n",
      "Average Balanced Accuracy: 0.167\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80     11085\n",
      "           1       0.00      0.00      0.00       438\n",
      "           2       0.00      0.00      0.00       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.00      0.00      0.00       864\n",
      "           5       0.15      0.00      0.00      3415\n",
      "\n",
      "    accuracy                           0.67     16470\n",
      "   macro avg       0.14      0.17      0.13     16470\n",
      "weighted avg       0.48      0.67      0.54     16470\n",
      "\n",
      "gbc:\n",
      "Average Accuracy: 0.550\n",
      "Average Balanced Accuracy: 0.154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.78      0.72     11085\n",
      "           1       0.01      0.00      0.01       438\n",
      "           2       0.01      0.01      0.01       222\n",
      "           3       0.01      0.01      0.01       446\n",
      "           4       0.02      0.01      0.01       864\n",
      "           5       0.17      0.11      0.13      3415\n",
      "\n",
      "    accuracy                           0.55     16470\n",
      "   macro avg       0.15      0.15      0.15     16470\n",
      "weighted avg       0.48      0.55      0.51     16470\n",
      "\n",
      "dt:\n",
      "Average Accuracy: 0.556\n",
      "Average Balanced Accuracy: 0.155\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72     11085\n",
      "           1       0.04      0.01      0.02       438\n",
      "           2       0.01      0.01      0.01       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.04      0.03      0.03       864\n",
      "           5       0.15      0.08      0.10      3415\n",
      "\n",
      "    accuracy                           0.56     16470\n",
      "   macro avg       0.15      0.16      0.15     16470\n",
      "weighted avg       0.48      0.56      0.51     16470\n",
      "\n",
      "svm:\n",
      "Average Accuracy: 0.667\n",
      "Average Balanced Accuracy: 0.166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.99      0.80     11085\n",
      "           1       0.00      0.00      0.00       438\n",
      "           2       0.00      0.00      0.00       222\n",
      "           3       0.00      0.00      0.00       446\n",
      "           4       0.00      0.00      0.00       864\n",
      "           5       0.18      0.01      0.02      3415\n",
      "\n",
      "    accuracy                           0.67     16470\n",
      "   macro avg       0.14      0.17      0.14     16470\n",
      "weighted avg       0.49      0.67      0.54     16470\n",
      "\n",
      "Class Counts in Training Set:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_counts_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6968\\541755997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;31m# Print the class counts for training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Class Counts in Training Set:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mclass_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclass_counts_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Class {class_label}: {count} samples\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_counts_train' is not defined"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "# load the libraries that are required for this project:\n",
    "#---------------------------------------------------------------------------------------------\n",
    "import sys\n",
    "import time                     # Time is for estimating the computational time of every operation\n",
    "import pandas as pd\n",
    "import numpy as np              # NumPy is for numerical operations\n",
    "import matplotlib               # MatPlotLib is for making plots & figures\n",
    "import matplotlib.pyplot as plt # PyPlot is a subset of the library for making MATLAB-style plots\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "def grid_search(clf, params, X_train, y_train):\n",
    "    grid_search = GridSearchCV(clf, params, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Load the dataset\n",
    "path = r'C:\\Users\\lrm22005\\OneDrive - University of Connecticut\\Research\\emotion_graph\\codes\\EDA-graph\\\\'\n",
    "data = pd.read_csv(path + '\\\\traditional_lab_feature_matrix_labeled.csv')\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "X = data.drop(['class','valence','arousal','Class'], axis=1)\n",
    "y = data['class']\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = {\n",
    "    'nb': (OneVsRestClassifier(GaussianNB()), {'estimator__var_smoothing': [1e-6, 1e-5, 1e-4]}),\n",
    "    'knn': (OneVsRestClassifier(KNeighborsClassifier()), {'estimator__n_neighbors': [1, 2, 3, 4, 5], 'estimator__weights': ['uniform', 'distance'], 'estimator__p': [1, 2]}),\n",
    "    'rf': (OneVsRestClassifier(RandomForestClassifier()), {'estimator__n_estimators': [100, 200], 'estimator__max_depth': [20, 30], 'estimator__random_state': [42]}),\n",
    "    'abc': (OneVsRestClassifier(AdaBoostClassifier()), {'estimator__n_estimators': [50, 100, 150], 'estimator__learning_rate': [0.1, 0.5]}),\n",
    "    'gbc': (OneVsRestClassifier(GradientBoostingClassifier()), {'estimator__n_estimators': [50, 100], 'estimator__learning_rate': [0.1, 1.0], 'estimator__max_depth': [40, 60]}),\n",
    "    'dt': (OneVsRestClassifier(DecisionTreeClassifier()), {'estimator__max_depth': [10, 30], 'estimator__min_samples_split': [2, 10], 'estimator__random_state': [42]}),\n",
    "    'svm': (OneVsRestClassifier(SVC()), {'estimator__C': [1, 10, 100], 'estimator__gamma': [0.1, 1, 10], 'estimator__kernel': ['rbf']})\n",
    "}\n",
    "\n",
    "# Define the subjects\n",
    "subjects = X['subject']\n",
    "unique_subjects = X['subject'].unique()\n",
    "\n",
    "X = X.drop('subject', axis=1)\n",
    "\n",
    "# Define the subjects\n",
    "\n",
    "# Define the cross-validation strategy (leave-one-subject-out)\n",
    "cv = GroupKFold(n_splits=len(unique_subjects))\n",
    "\n",
    "# Perform grid search for each classifier\n",
    "best_classifiers = {}\n",
    "for name, (clf, params) in classifiers.items():\n",
    "    avg_accuracy = 0.0\n",
    "    avg_balanced_accuracy = 0.0\n",
    "    \n",
    "    # Initialize variables for evaluation\n",
    "    y_test_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    # Iterate over each subject as a separate test fold\n",
    "    for subject in unique_subjects:\n",
    "        # Identify the indices for the current subject in both features and target labels\n",
    "        subject_indices = subjects == subject\n",
    "        X_test, y_test = X[subject_indices], y[subject_indices]\n",
    "        X_train, y_train = X[~subject_indices], y[~subject_indices]\n",
    "        \n",
    "        # Grid search for the current classifier\n",
    "        best_clf = grid_search(clf, params, X_train, y_train)\n",
    "        best_classifiers[name] = best_clf\n",
    "        \n",
    "        # Train the best classifier on the training data\n",
    "        best_clf.fit(X_train, y_train)\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "            y_pred = best_clf.predict(X_test)  \n",
    "            \n",
    "        # Append current test data and predictions for overall evaluation\n",
    "#         y_pred = best_clf.predict(X_test)\n",
    "        y_test_all.extend(y_test)\n",
    "        y_pred_all.extend(y_pred)\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    avg_accuracy = accuracy_score(y_test_all, y_pred_all)\n",
    "    avg_balanced_accuracy = balanced_accuracy_score(y_test_all, y_pred_all)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.3f}\")\n",
    "    print(f\"Average Balanced Accuracy: {avg_balanced_accuracy:.3f}\")\n",
    "    print(classification_report(y_test_all, y_pred_all, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
